\chapter{Dark Matter}

\section{Introduction}
The standard model(SM) of particle physics has been extensively tested by a number of experiments. 
It is a quantum field theory consisting of three generations of quarks and leptons, two unbroken gauge symmetries $SU_C(3) \times U_Y(1)$ and a $SU_L(2)$ symmetry which is spontaneously broken by the scalar Higgs field.
Using the SM, a large number of phenomena can be accurately predicted, such as the scattering amplitudes and decay widths of the myriad of existing particles.
Some observations are not explained by the SM.
For instance, the standard model can not account for neutrino masses due to the apparent non-existence of right handed neutrinos, necessary for the Dirac mass term.
In another case, astrophysical and cosmological observations point towards the existence of copious amounts of ``cold dark matter" (CDM), a gravitationally interacting substance with little to no coupling to the electromagnetic field\cite{bertone_history_2018}.
The SM contains no viable candidates, and therefore CDM is presently one of the strongest pieces of evidence for beyond the standard model (BSM) physics.

The following sections will outline the outstanding evidence for CDM, and I will briefly explain the case for it being an extension to the standard model.
Observations across a wide range of distance scales indicate that a large component of the mass-energy density of the universe is in the form of a nonrelativistic, pressureless matter field feebly coupled to the electromagnetic field.
The only neutral SM particle stable on cosmological time scales is the neutrino.
Neutrinos which were thermally produced in the early universe are referred to as relics, and would be nonrelativistic today.
However, in order to explain the virial mass of galaxies, the maximum number density of dark matter particles possible from Fermi-Dirac statistics requires leptons with masses larger than $\approx 1$~MeV/c$^2$\cite{tremaine_dynamical_1979}.
Such a requirement excludes the upper limit on the neutrino masses set by the Planck collaboration ($\sum m_\nu < 0.12\mathrm{~eV}$)\cite{planck_collaboration_planck_2020}.
This is sometimes referred to as the Tremaine-Gunn bound\cite{tremaine_dynamical_1979}.
Dim celestial bodies such as  primordial black holes (PBHs) or brown dwarfs are another logical choice, however much of the mass range has been excluded from being the \textit{entire} dark matter density with microlensing and gravitational wave searches\cite{villanueva-domingo_brief_2021}.
An exception to this exists for PBHs of approximately asteroid mass ($\sim$10$^{18}$ kg).

An extension to the SM is therefore required to account for gravitational observations.
Either gravitational theory must be modified under certain circumstances, or a new, practically invisible particle must exist.
If CDM is massive particle, it is feebly coupled to the SM through some portal which also accounts for its initial production in the early universe.
It is through this small interaction with ordinary particles that experiments aim to discover its nature.
An overview of possible explanations for the required dark matter density and the methods for detection are presented below.

\section{Evidence for Dark Matter}
\subsection{Galaxy Clusters}
In 1933 Fritz Zwicky studied the velocities of galaxies within the Coma cluster.
These galaxies exhibited an abnormal amount of dispersion in recessional velocity compared to other clusters\cite{zwicky_fritz_rotverschiebung_1933}.
By estimating the distance to and radius of the Coma cluster, an order of magnitude estimate of the matter density was obtained.
Applying the virial theorem: 

\begin{equation}
    T = -\frac{1}{2}V~,
    \label{eq:virial}
\end{equation}
\noindent
where $T$ is the average kinetic energy, and $V$ is the average potential energy, a prediction for the velocity dispersion was found to be 80 km/s, whereas the measured velocity dispersion was 1000 km/s.
The increased velocity dispersion requires a density of luminous matter over 400 times larger than that estimated from observations.
This calculation was enough for Zwicky to note that the amount of ``dark matter" far exceeded that of luminous matter.
These estimates were later refined, but the field remained largely skeptical of the dark matter explanation.
A popular explanation at the time was the existence of an intergalactic medium (IGM), which existing techniques were unable to detect.
Modern studies indicate that the intergalactic medium  contributes 40-50\% of the baryonic matter of the universe\cite{bykov_equilibration_2008}.

\subsection{Galactic Rotation Curves}
Until the 1970s it was expected that the rotation curves of spiral galaxies asymptotically approached Kepler's third law ($(r\omega)^2 = GM/r$) as the distance of a particular star from the galactic core (i.e. the majority of the baryonic mass), $r$, increased. 
Deviations from Keplerian velocities were noticed fairly early but not fully understood until Rubin and collaborators utilized image tubes to collect data on hundreds of rotation curves, observing the Doppler shift in the 21~cm line.
The observed luminosities provided fairly accurate predictions for the velocities near the galactic nuclei, but they flattened, or even increased as a function of radius.
This is indicative of the vast majority of the mass of a galaxy coming in the form of a non-luminous ``halo" which extends far past the visible edge of the galaxy.
Since then, more modern technology has afforded surveys out to higher $z$ and $r$, confirming the initial results.

While not evidence for dark matter, an important result for understanding spiral galaxy rotation curves is the ``Tully-Fischer" relationship.
This is a power-law relation between the luminosity of a galaxy and the asymptotic rotation velocity. 
In the dark matter paradigm, this is interpreted as suggesting a proportionality (on average) between the amount of baryonic matter and the amount of dark matter in a galaxy.
However, it can also be interpreted as evidence for modified gravity, since the dynamics of the rotation curves could be predicted by a more complicated theory.


\subsection{Bullet Cluster}
\label{sec:bullet}
In 2006 what was referred to at the time as the ``smoking gun" evidence for dark matter was observed\cite{clowe_direct_2006}.
Galaxy cluster 1E 0657-56, sometimes referred to as the ``Bullet Cluster" consists of two apparent subclusters in the process of merging.
The bow shock in the intergalactic medium produces copious amounts of x-rays and affords estimation of the relative velocity of the subclusters.
The center of gravity, measured by the strong and weak lensing, was observed to be offset from the ``center of mass," measured by the x-rays and optical observations. 
This observation has given great credence to the particle dark matter theory,
which explains that the bulk of the gravitating mass was in the form of matter which did not have a strong self-interaction cross section.
In other words, the dark matter, making up the bulk of the mass in either subcluster, passes uneventfully through itself during the collision, while the baryonic matter experiences drag and slows down.
Limits were subsequently set on the dark matter self interaction cross  section\cite{randall_constraints_2008} at

\begin{equation}
    \frac{\sigma^2_\chi }{ M_\chi} < 1.25~\frac{\mathrm{cm}^2}{\mathrm g} ~.
\end{equation}

\subsection{Big Bang Nucleosynthesis}

Light elements were formed during the early universe in a short time window between when the temperature dropped below the 2.2 MeV dissociation temperature of the deuteron and the 14 minute lifetime of the neutron.
In that time, nearly all of the neutrons and protons are fused into deuterons, and those deuterons then nearly completely fuse into $^4$He, which yields a mass fraction $Y\approx 0.25$, consistent with observation.
The primordial value of the deuterium-to-proton ratio D/H from the leftover deuterons is  extremely sensitive to the baryon-to-photon ratio $\eta$.
Elements including $^3$He, $^6$Li, and $^7$Li  also form, allowing for independent measurements, as shown in Figure \ref{fig:bbn}.
The densities today are parameterized today in terms of the ratio to the critical density, where $\Omega_b = \rho / \rho_c$ is the normalized density of baryons.
 This process of \textit{Big Bang Nucleosynthesis}\cite{cyburt_big_2016}(BBN) estimates a value of $\Omega_b h^2 = 0.022305 \pm 0.000225$, or $\approx 4\%$ of the critical density.
 Here $H_0 = h \times 100\mathrm{~km}\cdot \mathrm{s}^{-1} \mathrm{MPc}^{-1}$, absorbing the uncertainty on the Hubble constant into $h$.
 A flat universe therefore requires some missing component to constitute the remainder of the critical density $\Omega_0 h^2$, suggesting the existence of a dark matter component.
 
 \begin{figure}
     \centering
     \includegraphics[width=0.7\textwidth]{Assets/darkmatter/474px-Schramm_plot_BBN_review_2019.png}
     \caption[Isotopic abundances predicted by Big Bang Nucleosynthesis.]%
     {Isotopic abundances predicted by BBN, figure from \cite{fields_big-bang_2014}.
    The CMB bounds are consistent with observations, validating BBN.
    There is a noticeable discrepancy with the $^7$Li abundance, known as the ``Cosmological Lithium-7 Problem"\cite{bertulani_big_2022}.
    }
     \label{fig:bbn}
 \end{figure}
\subsection{Cosmic Microwave Background Radiation}
\subsubsection{Big Bang Cosmology}
Under the assumptions of homogeneity and isotropy, an expansion of an $N$-dimensional space can be modelled as a surface of constant curvature $k$ embedded within an $N+1$-dimensional space. 
The curvature $k$ is related to the difference between the circumference of a circle in the curved space and a circle of identical radius in Euclidean geometry. 
As the universe expands, the distance between two points $R(t)$ increases with time. 
This expansion is factored out into a scale factor $a$ and comoving distance $r$, $R(t) = a(t) r$, with $a_0=a(t_0)=1$ being the scale factor today.
With these assumptions, one obtains the FLRW metric:

\begin{equation}
    ds^2 = c^2dt^2 - a(t)^2 [ \frac{dr^2}{1-kr^2} +  r^2 d\Omega^2]~,
    \label{eq:flrw}
\end{equation}
\noindent
with $d\Omega^2 = d\theta ^2+ \sin^@ \theta d \phi^2$ being the solid angle.
The curvature can be reparameterized such that the available options are $k=\{-1,0,1\}$.
By inserting this equation into the Einstein field equations from general relativity, the Friedmann equations are obtained, which describe the evolution of the scale parameter $a$ depending on the pressure $p$ and density $\rho$ of the universe.

\begin{align}
        (\frac{\dot a}{a})^2 + \frac{k c^2}{a^2} = \frac{8 \pi}{3} {G \rho} \\
 \frac{\ddot a}{a}  = -\frac{4 \pi}{3c^2} G (\rho c^2 + 3p)~,
 \label{eq:friedmann}
\end{align}
\noindent
where $G$ is the gravitational constant, $\rho$ is the energy density, and $p$ is the pressure.
In the absence of curvature $k=0$, we can define the Hubble expansion rate $H \equiv \dot a /a $, as well as the critical density $\rho_c$ as the density which provides the Hubble rate today $H_0$:
\begin{align}
    \rho_c = \frac{3H_0^2}{8 \pi G}~.
\end{align}

Under-or-over densities $\Omega=\rho/\rho_c \neq 1$ thereby correspond to changes in the curvature parameter $k$.
Inserting a particular equation of state, or relationship between pressure and density, the Friedmann equations can be solved for $a(t)$.
In the early universe radiation was initially dominant, with $p = \rho c^2 /3$, followed by a period of matter domination, with $p=0$.
These lead to expansion of $a\sim t^{1/2}$ and $a\sim t^{2/3}$, respectively.

As the universe expanded, the radiation density fell as $a^{-4}$, whereas the energy density of the matter fell like $a^{-3}$. 
Two important events happen as a consequence of this. 
First, the density of radiation and matter become equal at $z_{\gamma m}$.
Then, at a later time the temperature of the photon-baryon plasma drops low enough that neutral hydrogen can form, referred to as ``recombination."
At this point in time the universe became quasi-transparent to light, and now the surface of last scattering can be seen in the cosmic microwave background radiation (CMBR).

While subdominant in the early history of the universe, it is worth mentioning that an additional cosmological constant term may be added to the right hand side of the Friedmann equations, turning them into the following: 

\begin{align}
        (\frac{\dot a}{a})^2 + \frac{k c^2}{a^2} = \frac{8 \pi}{3} {G \rho} + \frac{\Lambda c^2 }{3} \\
 \frac{\ddot a}{a}  = -\frac{4 \pi}{3c^2} G (\rho c^2 + 3p)  + \frac{\Lambda c^2 }{3}~.
\end{align}
\noindent
This term provides a constant positive energy density and negative pressure.
In a flat universe $k=0$, when $\Omega_\Lambda$ dominates the matter and radiation terms $\Omega_m$ and $\Omega_r$, this leads to exponential expansion of the scale factor $a(t)$.

\subsubsection{CMB Anisotropies}
The CMBR is a near-perfect Planck spectrum at 2.7~K, uniform to $10^{-4}$.
The small observed anisotropy is dominated by the dipole moment, corresponding to the Doppler shift of the relative motion of the earth to the CMB.
After subtracting off the dipole component, and expanding into spherical harmonics, a distinct pattern of peaks and troughs is observed. 

Initial small density perturbations $\delta$ grow until they cross the \textit{sound horizon}, and  oscillate thereafter. 
The acoustic peaks thereby represent an alternating sequence of compression and rarefaction peaks which are frozen in time at the ``moment" of recombination.
Smaller peaks (larger multipole $l$) entered their horizons and started oscillating earlier than larger peaks (smaller $l$).

The standard model of cosmology, $\Lambda CDM$, which includes dark energy and dark matter, explains the salient features of the acoustic peaks, shown in Fig. \ref{fig:cmb}
While the overall model contains some degeneracies, some observables are explained relatively succinctly:

\begin{itemize}
    \item The location of the first compression peak at $l\sim200$. 
    This is most sensitive to the curvature parameter and is consistent with a flat spacetime.
    \item The relative height of the second (rarefaction) peak.
    Its relative amplitude is caused by an effect known as \textit{baryon loading}. 
    Because the photon-baryon system started at rest, the inertia of the baryons causes the equilibrium around which the oscillations occur to shift. 
    This causes the  compression peaks to be taller than the rarefaction peaks.
    This effect is predicated on the presence of potential wells for the baryons to fall into. 
    In a radiation dominated universe, the baryon-photon plasma forms these potential wells, which decay away as $a^4$, which removes the loading effect.
    Baryons also contribute to an effect called \textit{Silk damping}, where the finite scattering length as the plasma recombines results in the higher peaks being smoothes out relative to the lower peaks.
    This effect is therefore degenerate with $\Omega_m h^2$ up to two peaks, needing a third peak to determine with certainty.
    \item The height of the third peak relative to the second peak.
    This is  strong evidence for dark matter\cite{hu_cmb_2003}.
    As the radiation density decays away faster than the matter density, oscillations receive a strong driving force when matter-radiation equality occurs at $z_{eq}$.
    Overdensities which  enter the the sound horizon during radiation domination result in acoustic peaks $\sim 5\times$ higher than those entering the horizon at matter domination.
    The density of matter $\Omega_m h^2$ shifts the location of radiation-matter equality, with higher densities pushing the effect to higher multipoles.
    By acting as a replacement potential well, dark matter maintains the baryon loading effect, as mentioned above.
    Radiation-matter equality occurred  slightly before recombination, with $z_m \approx 3000$ and $z_{CMB}=1100$.
    Thus, the fact that the third peak is boosted relative to the second is strong evidence for dark matter.
    \item The damping tail.
    The extreme multipoles are suppressed due to the Compton scattering that occurs during the time between recombination and full transparency.
    This provides an independent cross check of the baryon density $\sigma_b$.
\end{itemize}

The observed heights of the acoustic peaks require additional gravitational effects in excess of what the baryons themselves can provide, confirming the existence of CDM.
The \textit{Planck} survey\cite{planck_collaboration_planck_2020} provides a high precision estimation of the CMB anisotropy.
When combined with other constraints on the cosmological parameters (such as Big Bang Nucleosynthesis), it favors a flat universe with cold dark matter density $\Omega_c = 0.267$, with $\Omega_\Lambda=0.7$ and $\Omega_b$ taking up the remaining $0.04$ of the critical density.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Assets/darkmatter/Planck_cmB.png}
    \caption[The Cosmic Microwave Background Radiation temperature anisotropy spectrum.]%
    {The Cosmic Microwave Background Radiation temperature anisotropy spectrum.
    Figure from Ref. \cite{planck_collaboration_planck_2020}.
    Not the relative height of the second and third peaks, which indicates baryon loading in the presence of gravitational wells formed by cold dark matter $\Omega_m h^2$.}
    \label{fig:cmb}
\end{figure}

\subsection{Large Scale Structure}

The relative uniformity of the CMB raises an important question, namely how the small perturbations can grow to form large scale structures such as galaxies and galaxy clusters.
In other words, why nonlinear density and structures exist with $\delta_\rho >1$, rather than modest perturbations $\delta_\rho <<1$ on top of a uniform matter field.
Features smaller than the mean free path of a photon will be damped during recombination, and the Hubble term prevents further collapse.
CDM resolves this by providing additional mass to continue collapsing into the nonlinear regime, leading to galaxies today.

The theory of gravitational collapse is described by the Jean's instability equation.
Assuming that the fluid can be described by a collisionless Boltzmann equation, the density perturbations $\delta = \rho -\rho_0$ evolves according to a wave equation:

\begin{equation}
    \frac{\partial^2 \delta}{\partial t^2} + 2H(t) \frac{\partial \delta}{\partial t} + \delta (k^2 c_s^2 - 4 \pi G \rho_0 )=0~.
    \label{eq:jeans}
\end{equation}
\noindent
Thus, solutions with at sufficiently small $k$(i.e. large $\lambda$) result in  solutions with $\omega^2 <0$, leading to exponential growth or decay.
This defines the Jeans length, and therefore the Jean's mass, above which the overdensities $\delta$ will grow over time.
\begin{align}
    \lambda_J = c_s (\frac{\pi}{G \rho_0})^{1/2} \\
    M_J = \frac{4 \pi}{3} (\frac{\lambda_J}{2})^3~.
\end{align}

Crucially, since $\rho_0$ diminishes at the scale factor $a(t)$ increases, the Jeans mass increases over time during radiation domination ($c_s=c/3$ during this time).
Thus, overdensities will grow until until $M_J$ overtakes them, or equivalently when they enter the sound horizon. 
Larger fluctuations will grow for a longer amount of time than smaller fluctuations.
Between matter-radiation equality and recombination, photon diffusion will damp out smaller fluctuations.
When recombination finally occurs, the speed of sound rapidly drops, allowing for smaller fluctuations to once again grow.
However, by this point larger $k$ have been suppressed, making galaxy scale structure formation impossible.
The Jean's mass in this time frame evolves as the following:

\begin{equation}
    M_J  \approx 8 \times 10^{30} (1+z)^{-3} \Omega_b h^2 M_\odot ~.
\end{equation}
\noindent
Nonbaryonic matter solves this problem by virtue of being able to grow in the epochs between matter-radiation equality and recombination.
Subhorizon fluctuations are suppressed before matter domination due to the effective ``friction" term resulting from the Hubble expansion $H = \dot a / a$.
After matter domination the $\delta$ will grow at all scales, and after recombination the baryons will fall into the potential wells formed by the cold dark matter, leading to the growth of galaxy-scale structures.
Following recombination $M_J$ contracts further due to $c_s \propto T^{1/2}$, causing features of diminishing size to collapse.
This ``bottom-up" formation has observable consequences for the way matter is distributed on supergalactic scales.

Overall, the results are encapsulated in the matter power spectrum $P(k)$, the Fourier transform of the two point correlation function.
The halting of growth between $t_H$ and $t_{\gamma m}$ leads to a power law suppression for $k> 1/x_h(t_{\gamma_m})$.
Primordial fluctuations $P(k)\sim k$ are convolved on top of this spectrum, leading to a particular distribution which is confirmed in CMB anisotropies, galaxy clustering, and Lyman-$\alpha$ forests, shown in Fig. \ref{fig:matter_power_spectrum}.

Overall, cold dark matter leads to a bottom-up, hierarchical distribution of structure in the universe.
Interestingly, warm dark matter models, which describe particles which decouple while relativistic while being nonrelativistic in the present day, lead to predictable distortions in the power spectrum above the free-streaming scale of the dark matter particles.
Particle masses above  $\sim 1$~keV remain consistent with large scale structure formation while smoothing out extremely small features.
The latter property is desirable since is resolved the ``core-cusp" problem, a discrepancy between simulated and observed density profiles in galactic nuclei. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Assets/darkmatter/MatterPowerSpectrum.png}
    \caption[The matter power spectrum, inferred from a variety of observations at different distance scales.]{The matter power spectrum, inferred from a variety of observations at different distance scales.
    Figure taken from Ref. \cite{chabanier_matter_2019}.
    The slope of the tail at large wavenumber is indicative of cold dark matter.
    Relativistic or ``warm" dark matter would result in larger free-streaming distances, resulting in increased attenuation of the matter power spectrum at smaller scales.}
    \label{fig:matter_power_spectrum}
\end{figure}
\afterpage{\FloatBarrier}
\section{Dark Matter Models}
\subsection{Thermal Relics}
Despite the necessary feeble coupling between dark matter and baryonic matter, some production mechanism in the early universe is assumed to be necessary. 
The simplest case is a 2 $\rightarrow$2 interaction between dark matter particles $\chi$ and standard model particles $f$, with some velocity-dependent cross section $\sigma(v)$.
Combined with the additional assumption that the dark matter was initially in thermal equilibrium with the baryonic matter, one can predict the density of dark matter in the present epoch.
Later, the annihilation process $2 \chi\rightarrow 2 SM$ freezes out when the reaction rate drops below the Hubble expansion rate.
This sequence of events can be described by the Boltzmann equation:

\begin{equation}
    \frac{ d Y}{dt} = T^3 \langle \sigma v\rangle [Y_{EQ}^2 - Y^2]~,
    \label{eq:boltzmann}
\end{equation}
\noindent
where $Y\equiv n/T^3$ is the dimensionless ``yield" of the particles in question, which is proportional to the ratio $n_\chi/n_\gamma$ when the universe was radiation dominated.
The quantity $Y_{EQ}$ is the equilibrium value of the yield at a particular point in time.
Replacing the time coordinate with $x\equiv m/T$, and assuming that the process of freezing our occurs during radiation domination $\rho\propto T^4$, the Boltzmann equation becomes: 

\begin{align}
    \frac{dY}{dx} = \frac{\lambda}{x^2}(Y_{EQ}^2 - Y^2)\\
    \label{eq:freezeout}
    \lambda \equiv \frac{m^3 \langle\sigma v\rangle}{H(x=1)}~.
\end{align}
\noindent
The new dimensionless parameter $\lambda$ describes the annihilation rate relative to the universe's expansion rate.
As the universe expands and cools $a \propto 1/T$, the thermal equilibrium number density falls rapidly.
For a nonrelativistic species the scaling relationship is set by the Boltzmann factor $\exp(-m/T)$.
When $x <<1$, $Y$ can track $Y_{EQ}$ because the species is able to self-annihilate quickly.
However, when $x>1$, the annihilation rate drops, and eventually $Y>>Y_{EQ}$, where the species is referred to as being ``frozen out."
The time freezeout occurs, $x_f$, is given approximately by equating the annihilation and Hubble expansion rates. 

\begin{equation}
    n_{EQ} \langle \sigma v \rangle = H(x_f)~.
\end{equation}
\noindent
The solution to this equation depends on $m$ and the velocity dependence of $\sigma$.
For $s-$wave annihilation, $\sigma$ is constant, and $x_f \approx 20$, yielding an approximate freezeout velocity of $v \approx c/4$.


The equation \ref{eq:freezeout} lacks an analytic solution, necessitating some approximations.
Assuming $Y>>Y_{EQ}$ at the time of freezeout, and that the velocity dependence of the annihilation cross section can be written as $\lambda=\lambda_0x^{-m}$, 
one obtains: 

\begin{align}
    \frac{dY}{dx} = -\frac{\lambda_0}{x^{2+n}}Y^2 \\
    Y_\infty = \frac{n+1}{\lambda_0}x_f^{n+1} ~,
\end{align}
\noindent
where in the second line the differential equation is integrated from $x = x_f$ to $x=\infty$ and the asymptotic yield is assumed to be much less than the yield at freezeout, $Y_\infty << Y_f$.
Plugging in the mass $m$, present temperature $T_0$, and taking into consideration the change in number of relativistic degrees of freedom $g$ between freezeout and today, one obtains 

\begin{align}
    \Omega_\chi = \frac{\rho_\chi}{\rho_c} = \frac{4 \pi^3}{45} \frac{8 \pi}{3 H_0^2} \frac{x_f}{\langle \sigma v\rangle } (\frac{T_0}{M_{pl}})^3 \frac{g_\star (0)}{\sqrt{g_\star (m)}}\\
    \Omega h^2 \approx 0.2 (n+1)\frac{x_f^{n+1}}{20} \frac{ 10^{-26} \mathrm{~cm}^3/s}{\langle \sigma v \rangle}~.
\end{align}

\noindent
One particularly popular dark matter model is the weakly interacting massive particle, or WIMP.
 When applying a Weak interaction annihilation for a 100~GeV scale particle, one obtains $\sigma \sim G_F^2 m_\chi^2 \approx 10^{-39} \mathrm{~cm}^2$.
 This coincidence, that the approximate dark matter density is obtained naturally from a weakly interacting particle, is referred to as the ``WIMP Miracle" and served to motivate the field of dark matter direct detection towards optimizing for its discovery.
 
 There are some critical boundaries to this analysis. 
 When  using the s-channel annihilation via the $Z$ boson to standard model particles, the matrix element is proportional to  $m_\chi^2$.
 Therefore, lighter masses will underannihilate, and account for more than the required dark matter density and overclose the universe.
 For a classical WIMP this occurs at 2--4~GeV and is referred to as the ``Lee-Weinberg bound," below which different theories are required.
 These theories are varied, but revolve around methods to enhance the annihilation cross section relative to the SM-DM scattering cross section.
 So the Lee-Weinberg bound is not a hard limit, but rather the expected edge of the ``WIMP miracle".
 
 Regardless of the annihilation channel, quantum mechanical scattering theory is expected to hold.
Using partial wave analysis, the maximum cross section that can be achieved at a given spin is

\begin{equation}
    \sigma_J \leq \pi \frac{2J+1}{k^2}~.
    \label{eq:partialwaves}
\end{equation}
\noindent
Since the relic density is inversely proportional to $<\sigma v>$ at the time of freezeout, and freezeout occurs at nonrelativistic velocities, this sets a maximum possible cross section consistent with the observed relic density.
Since $k \approx m_\chi v $, and $v$ is constrained by the dynamics of the Boltzmann equation to be $\approx c/4$, this equivalently sets a maximum dark matter mass under s-wave ($J=0$) scattering at\cite{griest_unitarity_1990}

\begin{equation}
    M_\chi < 340 \mathrm{~TeV} .
\end{equation}

\noindent
This limit is known as the ``unitarity bound" and is widely applicable to thermal relics. 
This bound may be avoided in the case of non-thermal relics (where the assumptions in the above analysis no longer apply), or composite dark matter (where the mass at freezeout differs from the mass today).

\subsection{MSSM}
 The standard model contains ultraviolet divergences which require large degrees of fine-tuning to cancel out to the Planck scale\cite{shan_minimal_2003}.
 Supersymmetry solves this issue by placing all fundamental particles into either a gauge or chiral supermultiplet.
 Superpartners of standard model particles have differing spin, i.e. fermions are superpartners with bosons.
 This transformation leads to the desired cancellation as fermion loops present in the perturbative expansion are paired with boson loops of opposite sign.
 The winos and binos (the superpartners of the W and B bosons), along with the higgsino can mix, as the W and B do in the standard model, to form a new state, the neutralino $\tilde \chi_1^0$.
 
 The lightest supersymmetric partner (LSP), which in the minimal supersymmetric extension to the standard model (MSSM) is a dark matter candidate under certain circumstances. 
 MSSM by convention only adds terms to the Lagrangian necessary for the UV completion.
 To protect against baryon number and lepton number violation, a new discrete symmetry, \textit{R-parity}, is introduced\cite{jungman_supersymmetric_1996}.
 All particles have a new quantum number: 
 \begin{equation}
     P_R = (-1)^{3(B-L)+2s}~.
 \end{equation}
 \noindent
 Every term in the Lagrangian must preserve $P_R$, which guarantees the stability of the LSP, a desirable feature for a dark matter candidate. 
Tunable parameters include the fraction of each superpartner in the neutralino state, \textit{e.g.} mostly bino, wino, or higgsino.
Variations such as the \textit{Constrained} MSSM\cite{kane_study_1994}, have been extensively tested, and the surviving parameter space in that case is limited to the single TeV scale \cite{ellis_cmssm_2022}.


\subsection{Axions and axion-like-particles}

The Strong CP problem is the apparent fine tuning of the CP-violating terms in the QCD Lagrangian to nearly zero\cite{chadha-day_axion_2022}:
\begin{equation}
    \mathcal{L}_{\textit{CP}} = \bar \theta \frac{g^2}{32 \pi ^2}G_{\mu \nu}\tilde G^{\mu \nu}~,
\end{equation}
\noindent
where $g$ is the QCD coupling constant, $G_{\mu \nu}$
The neutron electron dipole moment, experimentally constrained to be $|d_n| <1.8\times 10^{-26} \mathrm{e} \cdot \mathrm{~cm}$\cite{abel_measurement_2020}, is sensitive to the value of $\bar \theta$.
Axions are a potential solution to this problem which promotes the $\bar \theta$ parameter to a dynamical variable, and introduces a global $U(1)$ symmetry\cite{peccei_mathrmcp_1977}.
The axial Peccei-Quinn symmetry is spontaneously broken, which results in the Goldstone modes acquiring a mass, becoming the pseudoscalar axions $\phi_a$.
The axion mass and couplings are constrained to $m_a f_a \approx (10^{10} \text{eV})^2$.

The axion is a dark matter candidate by virtue of the \textit{misalignment mechanism}\cite{co_axion_2020}.
There, the initial value of the $\theta$ field is displaced from the minimum of the potential, and oscillations begin when $m_a\sim3H$.
The sub-eV mass scale of axions results in enormous number densities if it constitutes the entirety of the dark matter density.
These searches are therefore optimized for more wavelike dark matter models, examining resonant production.
Experiments such as ADMX\cite{khatiwada_axion_2021} and DM-radio\cite{silva-feaver_design_2016} search for resonances in superconducting solenoids via the inverse Primakoff effect.

When the strict relationship between the $m_a$ and $f_a$ is relaxed, the model no longer solves the strong CP problem, cut could still be a viable dark matter model.
In this case the particle is an ``axion-like particle" (ALP).
ALPs can couple to electrons and lead to an ER signal in direct detection experiments\cite{takahashi_xenon1t_2020}.

\subsection{Sterile Neutrinos}
Neutrinos decoupled from the thermal bath in the early universe, similar to the relic photons which constitute the Cosmic Microwave Background.
While the relic neutrinos constitute some matter density as they have some minimum mass splitting $\Delta m_{31}^2\approx 2.4\time10^{-3}$eV$^2$\cite{petcov_13_2012}, they can not explain the full density due to Tremaine-Gunn bound\cite{tremaine_dynamical_1979}.
This bound constrains the dark matter mass to $>1\mathrm{~MeV}$ from the Fermi-Dirac statistics within the galactic halo.
Thermal relic neutrinos have a density today\cite{giunti_fundamentals_2007}: 

\begin{equation}
    \Omega_\nu h^2 = \frac{\sum_i m_{\nu i}}{94.14 \mathrm{~eV}}~,
\end{equation}
\noindent
where $m_{\nu i}$ is the mass of the $i$-th neutrino eigenstate.
The necessary DM density requires an $\approx$10 eV scale neutrino, in tension with the CMB result\cite{planck_collaboration_planck_2020}.
Additionally such light states result in attenuation of large scale structure formation, inconsistent with observation.

Since all active neutrinos are left handed, neutrino masses require some extension of the standard model.
Sterile, right handed neutrinos provide this through the seesaw mechanism\cite{boyarsky_sterile_2019}.
In a type-I seesaw the left- and right-handed neutrinos mix, leading to a mass matrix which, when diagonalized, forces the active species to be light and the sterile species to be heavy.
The sterile neutrinos would be produced via oscillation and would never be in thermal equilibrium (so-called ``freeze-in").

A mass of~keV scale leads to \textit{Warm Dark Matter}, which is produced relativistic but is today nonrelativistic.
It suppresses structure formation on distances smaller than its free streaming distance, which for 1~keV is approximately 1 MPc.
A 3.55~keV X-ray line from nearby galaxies has also been observed\cite{bulbul_detection_2014}.
However, further analysis of X-ray lines has lead to constraints on sterile neutrino dark matter\cite{dasgupta_sterile_2021}.

Due to the feeble couplings to the standard model, direct detection via sterile neutrino scattering off of nucleons is not competitive.

\subsection{Primordial Black Holes}

Black holes which form in the early universe, as opposed to stellar collapse, are known as \textit{Primordial Black Holes} (PBHs).
The mass of the PBHs is proportional to the mass contained in the Hubble horizon at the time of formation\cite{villanueva-domingo_brief_2021}, which is typically in the radiation-dominated era.
PBH are an attractive dark matter model as they may be small enough to lack detectable accretion disks and (fortunately) large enough to only occasionally pass close to the earth.
Stellar mass PBHs $M_{ \mathrm PBH} > M_\odot$ are excluded by 21~cm line analysis \cite{villanueva-domingo_21_2021}, gravitational waves, and CMB anisotropy.
Microlensing searches which exclude MACHOs\cite{hawkins_new_2015} force much of the mass parameter space to make up only a subcomponent of dark matter, usually less than 10$^{-2}$.
PBHs of $M<10^{17}$g will have evaporated by now.
If PBHs do constitute the entirety of the dark matter density $\Omega_\chi h^2\approx 0.12$ then their mass must lie in the range $[10^{-16}, 10^{-12}]M_\odot$\cite{villanueva-domingo_brief_2021}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Assets/darkmatter/PBH_limits.png}
    \caption[Current exclusion limits for primordial black holes in mass and fraction of dark matter density.]
    {Current exclusion limits for primordial black holes in mass and fraction of dark matter density.
    Figure from \cite{villanueva-domingo_21_2021}.
    A small gap of masses is still allowed to constitute the entirety of the dark matter density.
    }
    \label{fig:my_label}
\end{figure}

\subsection{Dark Sector}

Weak-scale interactions, mediated through the Higgs or the Z-boson, places constraints such as the above Lee-Weinberg bound, limiting dark matter to masses above a few~GeV.
To escape such bounds a more flexible theory is required.
These generically introduce a fermion or family of fermions  $\chi_i$ to populate the dark sector, along with a \textit{portal} which would couple the DS to the SM.
A vector-like portal would be the result of a new, broken U(1) symmetry, which may kinetically mix with the SM photon:

\begin{equation}
    \mathcal{L} \supset \frac{1}{2} m^2_{A'} A'^\mu A'_\mu + \epsilon e A' J_\mu^{EM}~,
\end{equation}
\noindent
where $m_{A'}$ is the mass of the dark photon, $A_\mu'$ is the dark photon field, $\epsilon$ is the mixing parameter, and $J_\mu^{EM}$ is the electromagnetic current.
This would result in millicharged dark matter\cite{berlin_dark_2019}.
Scalar dark sector mediators have the following additional Yukawa interactions: terms\cite{coskuner_direct_2019, essig_dark_2013}:
\begin{equation}
    \mathcal{L} \supset \frac{1}{2} m_\phi^2 \phi^2 + g_\chi \phi \bar \chi \chi + g_n \phi \bar n n + g_e \phi \bar e e~.
\end{equation}
\noindent
The mediator mass becomes a new parameter of the model.
With heavy mediators their mass is integrated out, and the cross section becomes dependent on $g^2/m_{phi}^2$.
For light mediators the interaction range is longer, and therefore a dependence on recoil momentum $F(q) \propto q^{-2}$ is introduced.

\subsection{Ultraheavy Dark Matter}
Thermal relic dark matter dark matter requires $\langle\sigma v_{rel}\rangle~\approx 3\times 10^{-27}$~cm$^3$/s, which sets a minimum necessary cross section at the freezeout velocity $x\approx 25 \rightarrow v_{rel}^2/4  \approx 1/16$.
Partial wave expansion saturates for s-waves at a mass of\cite{griest_unitarity_1990}
\begin{equation}
    m_\chi \leq 340 \mathrm{~TeV} \nonumber~.
\end{equation}
\noindent
Circumventing this limit requires some additional effects.
Sommerfeld enhancement can lead to enhanced annihilation which yields the correct density today.
Freeze-in models\cite{kolb_wimpzillas_1998} can generate large masses by not being subject to thermal freeze-out.
Asymmetric models\cite{zurek_asymmetric_2014} achieve the correct density by assigning a baryon number to dark matter, allowing the matter-antimatter asymmetry to be communicated to the dark sector.
If dark matter forms bound states from constituents\cite{gresham_astrophysical_2018} then the problem of achieving the correct relic density is decoupled from the mass of the dark matter state observed today.
More details can be found in Chapter \ref{chap:tracks}.
\afterpage{\FloatBarrier}
\section{Experiments}
\subsection{Production}
The first prong of dark matter searches involves the production process $2 SM \rightarrow 2 \chi$.
The procedure involves colliders and accelerators creating dark matter particles, which then escape the system.
The analysis consists of reconstructing the momenta of the jets in the interaction, and searching for interactions with deficits in momenta.
Collider limits are highly model-dependent, but are mostly sensitive to light ($<$10~GeV) masses with heavy mediators\cite{billard_direct_2022}.

\subsection{Indirect Detection}
Another prong of dark matter searches is the indirect route, $2 \chi \rightarrow 2 SM$.
While dark matter froze out soon after the big bang, regions of high density may still be able to annihilate dark matter at rates high enough to be observable, but low enough to still exist today.
The signal would therefore be a peak  or rise in the particle spectra at $m_\chi$.
This situation may arise in the dense core of galaxies, and the detected particles can in principle be any standard model particle lighter than $m_\chi$.
Fermi-LAT  \cite{collaboration_fermi_2017, hooper_dark_2011} observes an excess of gamma rays from 1~GeV out to 100~GeV coming from the galactic center.
The ``galactic center excess" is not claimed as dark matter due to uncertainties related to emission components above and below the galactic center (a.k.a the \textit{Fermi} bubbles). Limits have been set using galaxy-cluster $\gamma$-ray data which thermal relic dark matter as a source out to 100~GeV\cite{chan_ruling_2017}.
However, these excesses have not been confirmed and are in tension with blank-sky measurements\cite{dessert_dark_2020}.

\subsection{Direct Detection}
\subsubsection{Recoil Rates}
In a dark matter direct detection experiment, one is primarily interested in the number of events per unit mass of the detector medium.
 The initial assumptions are that the incident particles follow some velocity distribution $f(v)$, and that the cross section can be calculated in terms of the recoil energy $E_r$.
 \begin{equation}
     \frac{dR}{dE_r} = \frac{n_\chi}{m_T}\int dv v f(v) \frac{d \sigma_{\chi T}}{dE_r}~,
 \end{equation}
 \noindent
 where $n_\chi$ is the DM number density, $m_T$ is the mass of the recoiling nucleus, $v$ is the DM velocity, $\sigma_{\chi T}$ is the DM-nucleus cross section, and $E_r$ is the kinetic energy of the recoiling nucleus.
 From here, we make an assumption of the form of $\frac{d\sigma_{\chi T}}{dEr}$.
 Working in the nonrelativistic regime, the generic cross section is given by:
 
 \begin{equation}
     \frac{ d\sigma}{d \Omega} \approx \frac{|\mathcal{M}|^2}{64 \pi E_{\mathrm{cm}}^2} = \frac{g^4}{M_\Lambda^4} \frac{m_\chi^2 m_T^2}{(m_\chi+m_T)^2}  \propto \mu_{\chi T}^2~,
 \end{equation}
 \noindent
 where $\mathcal{M}$ is the matrix element, $E_{\mathrm{cm}}$ is the center of mass energy, and $\mu_{\chi T}$ is the reduced mass of the DM-nucleus system.
 Isotropic scattering implies that the recoil energies are uniformly distributed from 0 to the maximum kinematically allowed energy $rm_\chi v^2/2$, where $r = 4 m_\chi m_T /(m_\chi + m_T)^2$ is the maximum fraction of DM kinetic energy which can be transferred in the backscatter case.
 Instead of writing limits in terms of a coupling constant $g$ and mediatior mass $M_\Lambda$, spin-independent direct detection results are quoted in terms of a model-independent WIMP-nucleon cross section $\sigma_{\chi p}$ and scaled to the appropriate target.
 This allows experiments utilizing different targets to compare limits.
 \begin{equation}
     \sigma_{\chi T} = A^2 |F(q)|^2  \frac{\mu_{\chi T}^2}{\mu_{\chi p}^2}\sigma_{\chi p}
 \end{equation}
 
 Where $A$ is the atomic number of the target and $F_(q)$ is the form factor of the target, encapsulating the loss of coherent enhancement.
 Putting this together, one obtains:
  \begin{equation}
     \frac{dR}{dE_r} = A^2 |F(q)|^2 \sigma_{\chi p} \frac{\rho_\chi}{2m_\chi \mu_p^2} \int_{v_{min}(E_r)}^{v_{esc}} dv \frac{f(v)}v~,
     \label{eq:wimpscattering}
 \end{equation}
 \noindent
 where $v_{min}(E_r)$ is the minimum velocity of the incident particle which can result in a recoil energy $E_r$.
 The scaling features favor matching the target mass with the WIMP mass.
 For example, Xenon($A \sim 131$) is most sensitive at approximately $m_\chi=$~40~GeV.
 The asymptotic behaviour as $m_\chi \rightarrow \infty$ can be seen, as $\mu_p\rightarrow m_p$, and the overall differential recoil rate  scales as $\sigma_{\chi p} / m_\chi$.
 This degeneracy leads to an inability for direct detection experiments to distinguish high mass WIMP models from one another. 
 In Xenon, masses above 100~GeV/c$^2$ generate nearly identical recoil spectra.
Example recoil spectra are shown in Figure \ref{fig:recoil_rates}.

\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{Assets/Sims/Recoil_Rates.pdf}
    \caption[Differential recoil rates for assorted WIMP models with a WIMP-nucleon cross section of 1~zb.]{Differential recoil rates for assorted WIMP models with a WIMP-nucleon cross section of 1~zb(zeptobarn, $=10^{-45}$~cm$^2$).
    ``SI" denotes spin independent interactions, while ``SDn" and ``SDp" denote spin-dependent interactions with neutrons and protons, respectively.
    The SI limits are scaled downwards due to the fact that Xenon nuclei have at most one unpaired nucleon spin, eliminating much of the coherent enhancement present for spin-dependent limits.
    Each model is presented with two WIMP masses: 10~GeV/c$^2$ and 100~GeV/c$^2$.
    Both models show a mostly featureless falling exponential spectrum, but the larger mass has scatters out to larger recoil energies due to the better kinematic matching.}
    \label{fig:recoil_rates}
\end{figure}

 \subsubsection{Halo Model}
 The velocity distribution $f(v)$ is determined by the dynamics of the galactic halo. 
 This is typically assumed to be a Maxwell-Boltzmann distribution with dispersion $v_0$.
 The laboratory velocity is then the earth velocity in galactic coordinates, which is the sum of the local standard of rest, the sun's peculiar velocity, and the velocity of the earth around the sun. 
 Together, this model is referred to as the ``standard halo model" and its probability distribution is plotted in Fig. \ref{fig:shm}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Assets/darkmatter/Halo.pdf}
    \caption[The lab frame velocity probability distribution function of the standard halo model]%
    {The lab frame velocity probability distribution function of the standard halo model \cite{mccabe_earths_2014, baxter_recommended_2021}.
    Note the Gaussian shape, peak at approximately 300~km/s, and the tail extending out to 800~km/s.
    }
    \label{fig:shm}
\end{figure}

\subsubsection{Crystal Experiments}

Solid state detects measure energy deposits which promote electrons from the valence band into the conduction band.
Crystals may be grown with small, eV scale band gaps, and by operating at mK temperatures SuperCDMS has achieved a single e-h pair sensitivity in their 0.93~g sensor\cite{supercdms_collaboration_constraints_2020}.
Either the ionization signal is detected, as is the case with (Super)CDMS, or the crystal is doped such that the electrons are trapped and emit a scintillation photon upon deexcitation, as in the NaI-based DAMA\cite{bernabei_dark_2021} and the calcium-tungstate based CRESST\cite{angloher_results_2016}.

Some experiments have also started to utilize the Migdal effect in WIMP-nucleon scattering in order to search for lower masses, such as the Ge-based Edelweiss\cite{edelweiss_collaboration_search_2022}, which set competitive limits in the 10-100~MeV/cm$^2$ range.
This is a phenomenon predicted to occur for low-energy nuclear recoils, whereby the transient displacement of the recoiling nucleus within its own electron cloud results in either direct ionization or de-excitation x-rays\cite{ibe_migdal_2018}.

Concurrently, Edelweiss and similar experiments have started searching for DM-electron coupling\cite{gascon_low-mass_2022}, which benefits greatly from the reduced energy threshold.
Experiments of this type are commonly bolometers, which detect the phonons (heat) produced by scatters.
The Neganov-Luke effect produces a gain in phonons as the electrons are pulled through the crystal.
Skipper-CCDs are a leading technology in this mass range. 
SENSEI\cite{barak_sensei_2020} utilizes this technology to non-destructively read the integrated charge in each pixel, which yields sub-electron noise accuracy.
Their limits are world-leading below a mass of 10~MeV/c$^2$.

Surface backgrounds are a problem for this technology due to their small size not benefiting from self shielding.
However their low thresholds allow them to perform annual and diurnal modulation searches.

\subsubsection{Noble Liquid Experiments}

Noble liquid experiments measure  charge and light signals.
Time-projection chambers (TPCs) have the advantage of being monolithic and relatively starighforward to increase in target mass from kg to ton scale.
They have superior kinematic matching for high mass dark matter, while having higher energy thresholds which limit sensitivity to sub-~GeV dark matter.
For example, LZ sets the world's best exclusion limits for 40~GeV/c$^2$ dark matter\cite{aalbers_first_2022}.
More details can be found in Chapter \ref{chap:lz}.

\subsubsection{Directional Detection}

A smoking gun signal for dark matter would be a directional signal coming from the direction of the prevailing WIMP wind at declination 42$^\circ$.
In liquid noble TPCs the 4~$\mu m$ thermalization length of electrons, along with the S2 position resolution, makes it challenging to determine the recoil direction.
High pressure gases have the potential to balance the benefits of a dense medium (expense, energy threshold) with the ability to reconstruct long tracks and scale to extremely large sizes.
The CYGNUS experiment is exploring the possibility of building a He and SF$_6$ based detector at atmospheric pressure\cite{vahsen_cygnus_2020}.

